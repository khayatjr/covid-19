{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Imports**"},{"metadata":{"trusted":false},"cell_type":"code","source":"import os\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the data"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge/'\n\ndirs = [\n    root_path+'document_parses/pdf_json',\n]\ndocuments = []\nfor d in dirs:\n    for file in tqdm(os.listdir(d)):\n        j = json.load(open(d+f\"/{file}\",\"rb\")) \n        \n        title = j['metadata']['title']\n        \n        abstract = \"\"\n        if len(j['abstract']) > 0:\n            abstract = j['abstract'][0][\"text\"]\n            \n        text = ''\n        for t in j[\"body_text\"]:\n            text += t['text'] + \"\\n\\n\"\n            \n        documents += [[title, abstract, text]] \n\ndf = pd.DataFrame(documents, columns=[\"title\", 'abstract','text'])\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing google word2vec the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"import gensim\n\npath = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\"\n\nmodel = gensim.models.KeyedVectors.load_word2vec_format(path,binary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the TF-IDF vectorizer"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = df['text']\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit(corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Data Processing"},{"metadata":{},"cell_type":"markdown","source":"## Abstracts text processing \nApplying basic text processing techniques to all abstracts to prepare them for further processing."},{"metadata":{"trusted":false},"cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nprocessed_abstracts = []\nfor abstract in tqdm(df['abstract'].tolist()):\n    # Normalization\n    normalized = re.sub(r\"[^a-zA-Z0-9]\", \" \", abstract.lower())\n\n    # Tokenization\n    words = word_tokenize(normalized)\n\n    # Removing stopwords\n    no_stopwords = [w for w in words if w not in stopwords.words(\"english\")]\n\n    # Stemming \n    p = PorterStemmer()\n    stemmed = [p.stem(w) for w in no_stopwords]\n\n    processed_abstracts += [stemmed]\n\ndf['processed_abstracts'] = [ \" \".join(x) for x in processed_abstracts ]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrieving relevant documents using keywords"},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color: gray\"> Get all documents with abstracts containing a given keyword </span>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def getDocumnetsContain(keyword):\n    return df[df['abstract'].str.contains(keyword)]\n\ngetDocumnetsContain('pregnancy').head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color: gray\"> Get all sentences from all documents which their abstracts contain a given keyword </span>"},{"metadata":{"trusted":false},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\ndef getSentencesContain(keyword):\n    def sentContain(x):\n        # tokenize sentences \n        sentences  = sent_tokenize(x)\n        # filter el conatins el keyword\n        return [sent for sent in sentences if keyword in sent]\n\n    return np.array(list(map(sentContain, np.array(getDocumnetsContain(keyword)['text']))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"getSentencesContain(\"risk\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color: gray\"> Get documents in which  given keyword is repeated N or more times. </span>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def getDocumentsAbout(keyword, limitCount=10):\n    sents = getSentencesContain(keyword)\n    docs = getDocumnetsContain(keyword)\n    return docs[[(len(s)>limitCount) for s in sents]].iloc[:, 0:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"getDocumentsAbout('smoking', 5).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color: gray\"> Get document with the highest frequency of a given keyword </span>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def getDocumentWithMostFreq(keyword):\n    index = vectorizer.get_feature_names().index(keyword)\n    docs = getDocumentsAbout(keyword)\n    if(len(docs) > 0):\n        maxDoc = np.argmax(vectorizer.transform(docs['text'])[:,index].toarray())\n    return docs.iloc[maxDoc] if (len(docs) > 0) else \"No documents found\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"getDocumentWithMostFreq(\"pregnancy\")['title']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get top N documents most relevant to a given sentence using TF-IDF for relevance measure"},{"metadata":{"trusted":false},"cell_type":"code","source":"def getMostSimilarNDocuments(s, topNumber = 5):\n    from sklearn.metrics.pairwise import linear_kernel\n    sent = [s]\n    sentVec = vectorizer.transform(sent)\n    d = []\n    for word in s.split(' '):\n        d += [getDocumnetsContain(word)]\n    d = pd.concat(d, ignore_index=True)\n    docsVec = vectorizer.transform(d['text'])\n    cosine_similarities = linear_kernel(sentVec, docsVec).flatten()\n    indices = cosine_similarities.argsort()[:-topNumber:-1]\n    return df.iloc[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"getMostSimilarNDocuments('smoking risk factors pregnant drinking', 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the words most used with a given keyword using TF-IDF scores and visualize them."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Top Tfidf Score\ndef topUsedWordsWith(keyword, visualize=True):\n    docs = getDocumentsAbout(keyword)\n    if(len(docs) == 0): return \"No documents found\"\n    tfidf_result =  vectorizer.transform(docs['text'])\n    # http://stackoverflow.com/questions/16078015/\n    scores = zip(vectorizer.get_feature_names(),\n                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    count = 0\n    words = []\n    for item in sorted_scores:\n        if item[0] not in stopwords.words(\"english\"):\n            words += [(item[0], item[1])]\n            count+=1\n        if count > 20:\n            break\n    \n    k = words\n    names = [x[0] for x in k]\n    values = [x[1] for x in k]\n\n    plt.figure(figsize=(30, 7))  \n    plt.bar(range(len(names)),values,tick_label=names)\n    plt.show()\n    \n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"topUsedWordsWith('risk')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get documents relevant to a given keyword"},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color: gray\"> Get N most similar words to a given keyword </span>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def getSimilarWords(keyword, number = 3):\n     return model.most_similar(positive=[keyword], topn = number)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color: gray\"> Get N sentences which are relevant to a given keyword and its synynoms.\nFilter words can be applyed so returned sentences must contain them.</span>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def relevantSentenceQuery(keyword, filters = [], number = 3):\n    similar_keywords = [(keyword,0)] + model.most_similar(positive=[keyword], topn = number)\n    print(f\"similar words to '{keyword}'': {similar_keywords[0][0]}, {similar_keywords[1][0]}, {similar_keywords[2][0]}\")\n    result = []\n    for key in similar_keywords:\n        x = getSentencesContain(key[0])\n        result += getSentencesContain(key[0]).tolist()\n    flattened = []\n    for i in result:\n        for j in i:\n            add = True\n            for f in filters:\n                add *= (f in j)\n            if add:\n                flattened += [j]\n            \n    return list(set(flattened))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"relevantSentenceQuery('smoking', filters= ['risk', 'factor', 'progression','COVID-19'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the count of the words in the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nimport matplotlib.pyplot as plt\ndef flatten(l):\n    flattened = []\n    for i in l:\n        for j in i:\n            flattened += [j]\n    return flattened\ndef graphWordsCount(words):\n    counts = []\n    for w in words:\n        counts += [len(flatten(getSentencesContain(w)))]\n    \n    \n    k = words\n    names = words\n    values = counts\n\n    plt.figure(figsize=(30, 7))  \n    plt.bar(range(len(names)),values,tick_label=names)\n    # plt.savefig('bar.png')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"graphWordsCount([\"drinking\", \"pregnant\", \"drinking\"])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}